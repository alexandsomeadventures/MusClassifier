{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11869732,"sourceType":"datasetVersion","datasetId":7459197},{"sourceId":242454595,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from musdataset import model, optimizer, scheduler, criterion, last_epoch, num_epochs, PATH, dataloaders, val_patience\nimport torch.nn as nn\nimport numpy as np\nimport torchaudio\nimport torch\n\n\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# device_count = 1\n# device = xm.xla_device()\n# torch.set_num_threads(24)\n# checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice_count = torch.cuda.device_count()\nmodel = model.to(device)\nif device_count > 1:\n    model = nn.DataParallel(model)\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(device))\n\n\ndef adjust_state_dict_for_device(state_dict):\n    global device_count\n    \n    # Check if 'module.' prefix exists in any key\n    has_module_prefix = any(k.startswith('module.') for k in state_dict.keys())\n\n    if has_module_prefix and device_count <= 1:\n        # Remove 'module.' prefix\n        new_state_dict = {k.replace('module.', '', 1): v for k, v in state_dict.items()}\n        \n    elif not has_module_prefix and device_count > 1:\n        # Add 'module.' prefix\n        new_state_dict = {f'module.{k}': v for k, v in state_dict.items()}\n        \n    else:\n        # No change needed\n        new_state_dict = state_dict\n\n    return new_state_dict\n\n\ncheckpoint = torch.load(PATH)\nadjusted_state_dict = adjust_state_dict_for_device(checkpoint['model'])\nmodel.load_state_dict(adjusted_state_dict)\noptimizer.load_state_dict(checkpoint['optimizer']) \nscheduler.load_state_dict(checkpoint['scheduler'])\nlast_epoch = checkpoint['last_epoch']\n\n\ndef train(model, criterion, optimizer, scheduler, num_epochs, patience=val_patience):\n    global last_epoch\n    best_valid_acc = 0\n    bad_epochs = 0\n    for epoch in range(last_epoch, num_epochs):       \n        print('-' * 10)\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            correct = 0  \n            total = 0\n\n            for inputs, labels in dataloaders[phase]:\n                # with torch_xla.step():\n                    # inputs, labels = inputs.to('xla'), labels.to('xla')\n                    # print(\"TPU works\")\n                inputs = inputs.to(device, non_blocking=True)\n                labels = labels.to(device, non_blocking=True)\n                    \n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                        \n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    _, predicted = torch.max(outputs, 1)\n                        \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n    \n                correct += torch.sum(predicted == labels.data).item()\n                total += labels.size(0)\n            \n            # torch_xla.sync()\n            acc = correct / total * 100\n            \n            print(f\"{phase.capitalize()} Epoch {epoch+1}: {correct}/{total} ({acc:.2f}%)\")\n            \n            if phase == 'train':\n                scheduler.step()\n                \n            else:  # phase == 'valid'\n                last_epoch = epoch\n                \n                if acc > best_valid_acc:\n                    print(\"New best validation accuracy!\")\n                    best_valid_acc = acc\n                    bad_epochs = 0\n                    checkpoint = {\n                     'model': model.state_dict(),\n                     'optimizer': optimizer.state_dict(),\n                     'scheduler': scheduler.state_dict(),\n                     'last_epoch': last_epoch + 1\n                    }\n                    torch.save(checkpoint, PATH)\n                    \n                else:\n                    bad_epochs += 1\n                    print(f\"Validation accuracy did not improve. Bad epochs: {bad_epochs}/{patience}\")\n                    if bad_epochs >= patience:\n                        print(\"Early stopping triggered\")\n                        return\n    print(\"Training complete\")\n\n\ndef test(model):\n    model.eval()\n    with torch.no_grad():\n        correct = 0  \n        total = 0\n        for inputs, labels in dataloaders['test']:\n            inputs = inputs.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            outputs = model(inputs)\n            _ , predicted = torch.max(outputs, 1)\n        \n            correct += torch.sum(predicted == labels.data).item()\n            total += labels.size(0)\n        print(\"Correct/Total: \", correct, \"/\",total )\n        print(\"Accuracy: \", (correct/total) * 100, \"%\")\n    \n    print(\"Testing done\")\n    \n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(\"Number of parameters:\", total_params)\n\ndef prod(model = None): #for real demo purposes\n    pass\n\n\ntrain(model, criterion, optimizer, scheduler, num_epochs)\n\n# test(model)\n\n# prod(model)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T02:56:22.343331Z","iopub.execute_input":"2025-05-29T02:56:22.343512Z","iopub.status.idle":"2025-05-29T02:56:36.781241Z","shell.execute_reply.started":"2025-05-29T02:56:22.343490Z","shell.execute_reply":"2025-05-29T02:56:36.779861Z"}},"outputs":[{"name":"stdout","text":"False\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/935293379.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"],"ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","output_type":"error"}],"execution_count":1}]}