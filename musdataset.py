{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11869732,"sourceType":"datasetVersion","datasetId":7459197}],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [code]\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport torchaudio.transforms as T\nimport torch.optim as optim\nimport torch.nn as nn\nimport numpy as np\nimport torchaudio\nimport torch\nimport json\nimport os\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nPATH = './musclassifier.pth'\nmelspec = T.MelSpectrogram(n_mels = 32)\nmelspec = melspec\nbatch_size = 256\nnum_epochs = 8\nlast_epoch = 0 \nfeatures = 64\nclasses = ['bass','brass','flute','guitar','keyboard','mallet','organ','reed','string','synth_lead','vocal']\ncriterion = nn.CrossEntropyLoss()\n\n\nclass NSynthDataset(Dataset):\n    def __init__(self, inputs_dir, labels_dir, transform = None):\n        self.inputs_dir = inputs_dir\n        self.labels_dir = labels_dir\n        self.transform = transform\n\n        with open(self.labels_dir, 'r') as f:\n            self.labels = json.load(f)\n\n        self.labels_list = list(self.labels.items())\n       \n    def __len__(self):\n        return len(self.labels_list)\n\n    def __getitem__(self, idx):\n        filename, data = self.labels_list[idx]\n        \n        path = os.path.join(self.inputs_dir,filename +'.wav')\n        \n        waveform, sample_rate = torchaudio.load(path)\n        waveform = F.pad(waveform, (0, max(0, 64000 - waveform.shape[-1])))[:, :64000]\n        new_spec = melspec(waveform.mean(dim=0))\n        new_spec = torch.log(new_spec + 1e-10)\n        new_spec = new_spec.unsqueeze(0)\n\n        instrument = data[\"instrument_family\"]\n\n        return new_spec, instrument\n\ndataloaders = {\n    'train': DataLoader(NSynthDataset('/kaggle/input/nsynth-train/nsynth-train.jsonwav/nsynth-train/audio', \n                                      '/kaggle/input/nsynth-train/nsynth-train.jsonwav/nsynth-train/examples.json'), \n                        batch_size=batch_size, shuffle=True,num_workers=4,prefetch_factor=2,pin_memory=True),\n\n    'valid': DataLoader(NSynthDataset('/kaggle/input/nsynth-train/nsynth-valid.jsonwav/nsynth-valid/audio', \n                                      '/kaggle/input/nsynth-train/nsynth-valid.jsonwav/nsynth-valid/examples.json'), \n                        batch_size=batch_size, shuffle=True,num_workers=4,prefetch_factor=2,pin_memory=True),\n\n    'test': DataLoader(NSynthDataset('/kaggle/input/nsynth-train/nsynth-test.jsonwav/nsynth-test/audio', \n                                     '/kaggle/input/nsynth-train/nsynth-test.jsonwav/nsynth-test/examples.json'), \n                       batch_size=batch_size, shuffle=True,num_workers=4,prefetch_factor=2,pin_memory=True)\n}\n\nclass SelfAttention(nn.Module):\n    def __init__(self, dim, num_heads = 4, qkv_bias = False, proj_bias = True):\n        super().__init__()\n        assert dim % num_heads == 0\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        \n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim, bias=proj_bias)\n    def forward(self, x, mask=None):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n        out = F.scaled_dot_product_attention(q,k,v, mask)\n        out = out.transpose(1, 2).reshape(B, N, C)\n        out = self.proj(out)\n        return out\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, features, kernel_size = 3, stride = 1, padding=1)\n        self.bn1 = nn.BatchNorm2d(features)\n        self.conv2 = nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.attn1 = SelfAttention(128)\n        self.attn2 = SelfAttention(128)\n        self.fc1 = nn.Linear(128, 256)\n        self.dropout = nn.Dropout(0.4)\n        self.fc2 = nn.Linear(256, len(classes))\n        \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        \n        B, C, H, W = x.shape\n        x = x.permute(0, 2, 3, 1).reshape(B, H*W, C)\n        \n        x = self.attn1(x)\n        x = self.attn2(x)\n        \n        x = x.mean(dim=1)  # shape: (B, C)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CNN()\noptimizer = optim.AdamW(model.parameters(),lr=1e-3, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,num_epochs, eta_min=0, last_epoch = -1)","metadata":{"_uuid":"8dd566f8-1161-4766-ac45-c964d676fa61","_cell_guid":"e12133b0-d612-420f-80df-312fe9bacf41","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}